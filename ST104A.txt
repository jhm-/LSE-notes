Definiton of 'population'	The complete set of all items, defined as N
Definition of 'sample'	An observed subset (or portion) of a population, defined as n
Definition of 'systemic bias'	An estimation methodology that results in systemic errors, ie. the inherent tendency of a process to support particular outcomes, where the process consistently over-/under-estimates some population parameter
Definition of 'parameter'	A numerical measure that describes a specific characteristic of a population
The two key advantages of random sampling	i.) it avoid systemic bias<br>ii.) it allows an assessment of the size of the sampling error
(Four-part) definition of 'simple random sampling'	A procedure used to select a sample of n objects from a population so that: <br>i.) each member of the population is chosen by chance<br>ii.) the selection of one member does not influence the selection of any other member<br>iii.)  each member of the population is equally as likely to be chosen<br>iv.) every possible sample of a given size, n, has the same chance of selection.
The two most common kinds of non-simple random samples	i.) stratified<br>ii.) clustered
The (two) key advantages of random sampling	i.) it avoids systemic bias<br>ii.) it allows an assessment of the size of the sampling error
Definition of 'discrete data'	Things that can be counted, and may (but does not necessarily) have a finite number of values<br><br>DIscrete data can be numeric or categorical
Definition of 'continuous data'	Things that can be measured, which depends on the accuracy of the instrument to measure<br><br>Continuous data is always numeric
Definition of a 'census'	The total enumeration of a population, not a sample!
Two different types of categorical variables	i.) ordinal variables, which can be put in rank/order<br>ii.) nominal variables, which cannot be put in any sensible order and are known only by their names
Definition of 'histogram'	A graph that consists of vertical bars constructed on a horizontal line that is marked off with intervals for the variable being displayed.  The intervals correspond to the classes in a frequency distribution table, and the height of each bar is proportional to the number of observations in that interval.
What type of graph illustrates the frequency density of an observation?	A histogram, where the intervals/'bins' are not all of equal width, and the area of each bar represents the frequency.
Definition of a 'stem-and-leaf display'	A graph where data is grouped according to their leading digits, called stems, and final digits, called leaves, which are placed in order of magnitude within the stems<br><br>Unlike a histogram, the actual data values are preserved
What are the three principal measures of location in a probability distribution?	The mean, median and mode.
Definition of the 'sample mean' AKA 'arithmetic mean'	<$$>{\frac {\sum_{i=1}^{n} x_i}{n}</$$>
Definition of the 'weighted mean'	For data serts in frequency form, where [$]f_k[/$] is the frequency of observations and [$]x_k[/$] are the different variable values, the weighted mean is:<br><br>[$]\overline{x} = \frac{\sum_{k=1}^{n}f_k x_k}{\sum_{k=1}^{n}f_k}}[/$]
Definition of the 'median'	The median is the middle value of an ordered dataset, where observations are arranged in (ascending) order.  If the sample size is an even number, the median is the arithmetic average of the two middle values.
The conditions that produce a positive-/right-skewed distribution	Mean > Median > Mode indicates a positively-skewed distribution<br><br><img src="pearson-mode-skewness.jpg">
Definition of the 'mode'	The mode, if one exists, is the most frequently occuring value in a dataset, and is most often used with categorical data.<br>Unimodal = one mode<br>Bimodal = two modes<br>Multimodal = > two modes
Definition of the 'range'	The range is the difference between the largest and smallest values, and is a measure of spread (or dispersion).  Because it is very sensitive to outliers, we typically use the Interquartile Range
Definition of the 'interquartile range'	The interquartile range measures the spread in the middle 50% of the data; it is the difference between the observation at [$]Q_3[/$], the third quartile (or 75th percentile), and the observation at [$]Q_1[/$], the first quartile (or 25th percentile).<br><br>[$]IQR = Q_3 - Q_1[/$]
Definition of a 'boxplot'	A boxplot is a graph that describes the shape of a distribution in terms of the 5-number summary: the minimum value, first quartile, median, third quartile, and the maximum value.<br><br><img src="111714_1527_MethodsofMe3.png">
Definition of the 'sample varience'	The sample varience, [$]s^2[/$] is the sum of the squared differences between each observation and the sample mean divided by the sample size, n, minus 1.  The standard deviation is the square root of [$]s^2[/$], which restores the data to its original measurement units.
Definition of 'sample standard deviation'	A measure that is used to quantify the amount of dispersion in a set of data values.  A low standard deviation indicates that the data points tend to be close to the mean of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values.<br><br>[$]\sigma = \sqrt{s^2} = \sqrt{\frac{\sum_{i=1}^{n}(x_i - \overline{x})^2}{n - 1}}[/$]
Definition of 'sample space'	Sample space, [$]S[/$], is the set of all possible outcomes for an experiment.<br><br>eg. for a 6 sided die, [$]S = \{1, 2, 3, 4, 5, 6\}[/$]
Definition of an 'event'	An event is a collection of elementary outcomes from the sample space, [$]S[/$], of an experiment, and therefore is a subset of [$]S[/$]<br><br>Typically events are denoted by leters, eg. if [$]A[/$] represents an even score from a 6-sided die, then [$]A = \{2, 4, 6\}[/$]
Definition of a 'random experiment'	A process leading to two or more possible outcomes without knowing which outcome will occur.
Definition of 'probability' for an experiment with equally likely elementary outcomes	Let [$]N[/$] be the total number of outcomes and let [$]n[/$] be the number of outcomes that are favourable to our event of interest, [$]A[/$], then:<br><br>[$]P(A) = \frac{n}{N}[/$]
Definiton of the 'relative frequency of probability'	If an experiment is repeated an extremely large number of times and a particular outcome occurs a percentage of the time, then that particular percentage is close to the probability of that outcome (ie. approaching the limit).
The three axioms of probability	1. For any event, [$]A[/$],<br>[$]0 \leq P(A) \leq 1[/$]<br><br>2. For the sample space [$]S[/$],<br>[$]P(S) = 1[/$]<br><br>3. If [$]\{A_i\}_{i = 1}^{n}[/$] are mutually exclusive events, then the probability of their 'union' is the sum of their respective probabilities; that is:<br>[$]P(\cup_{i=1}^{n}A_i) = \sum_{i=1}^{n}P(A_i)[/$]
Definition of 'collectively exhausted'	If the union of several events cover the entire sample space, [$]S[/$], we say that the events are collectively exhausted.<br>ie. all possible experimental outcomes are included among the collection of events.
Definiton of a 'complement'	The set of basic outcomes of a random experiment belong to sapce space [$]S[/$] but not to [$]A[/$] is called the complement of [$]A[/$] and is denoted by [$]\bar{A}[/$]<br><br>So [$]A[/$] and [$]\bar{A}[/$] are mutually exclusive and collectively exhaustive
What are the set operations?	[$]<br>\usepackage{amsfonts}<br>\begin{center}<br>\begin{tabular}{c c c}<br>  Symbol & Meaning & Logical Operation \\<br>  \hline<br>  \cup & Union & Or \\<br>  \cap & Intersect & And \\<br>  A^c & Complement & Not \\<br>  \mid & Conditional & (None)<br>\end{tabular}<br>\end{center}<br>[/$]
The additive law	[$]P(A \cup B) = P(A) + P(B) - P(A \cap B)[/$]<br><br>If [$]A[/$] and [$]B[/$] are mutually exclusive, then:<br><br>[$]P(A \cup B) = P(A) + P(B) - 0[/$]
The multiplicative law	[$]P(A \cap B) = P(A) - P(B)[/$]<br><br>The multiplicative law does not hold for dependent events, which is the subject of conditional probability
The formulas for conditional probability	[$]<br>P(A \mid B) = \frac{P(A \cap B)}{P(B)}<br><br>P(B \mid A) = \frac{P(A \cap B)}{P(A)}<br><br>P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)<br>[/$]
Bayes formula	[$]<br>P(A \mid B) = \frac{P(B \mid A)P(A)}{[P(A)P(B \mid A)] + [P(\bar{A})P(B \mid \bar{A})]}<br>[/$]
Definition of 'random variable'	A variable that takes on numerical values realized by the outcomes in the sample space generated by a random experiment. This is a 'discrete random variable' if it can take on no more than a countable number of values, and a 'continuous random variable' if it is measurable and can take any value in an interval.
Expected value of a discrete random variable	If [$]x_1, x_2, ..., x_n[/$] are the possible values of the random variable [$]X[/$], with corresponding probabilities [$]p_1, p_2, ..., p_n[/$], then:<br><br>[$]E(X) = \mu_x = \mu = \sum_{i=1}^{n}p_i \times x_i[/$]
Varience of a discrete random variable	If [$]x_1, x_2, ..., x_n[/$] are the possible values for the random variable [$]X[/$], with the corresponding probabilities [$]p_1, p_2, ..., p_n[/$], then:<br><br>[$]\sigma^2 = E((x - \mu)^2) = \sum_{i=1}^{n}p_i \times (x_i - \mu)^2[/$]<br><br>And a derivation of this formula is:<br><br>[$]\sigma^2 = E(X^2) - \mu^2 = \sum_{i=1}^{n}(x_i^2 \times p_i^2) - \mu^2[/$]
The 'general form' of the equation which describes the normal distribution	[$]f(x) = \frac{1}{sqrt{2 \times \pi \times \sigma^2}} \times e^\frac{-(x - \mu)^2}{2 \times \sigma^2}[/$]<br><br>All you have to know for the exam is that it involves 2 parameters: the mean and the variance.
Definition of the 'central limit theorem'	Any function of sample variables, such as the averages of sample variables, also have probability distributions, and provided the sample size is large, the distribution of the sample mean, [$]\bar{x}[/$], will be approximately normal regardless of the distribution of the original variable.
Two consequences of the central limit theorem	1. A number of statistical methods have a robustness property.<br>2. We are justified in assuming normality for statistics which are sample means or linear transformations of them.
The transformation formula for standardization	If [$]X \sim N(\mu, \sigma^2)[/$], then the transformation:<br><br>[$]Z = \frac{X-\mu}{sigma}[/$]<br><br>creates a standard normal variable (ie. [$]Z \sim N(0,1)[/$]
Definition of an 'estimator'	A statistics, (which is a random variable), describing how to obtain a (point) estimate, (which is a real number), of a population parameter.<br><br>For example: [$]\bar{X} = \sum_{i=1}^{n}\frac{X_i}{n}[/$] could be the estimator for the population mean, [$]\mu[/$]
Definition of a 'sampling distribution'	The probability distribution of an estimator.<br><br>To clarify:<br><br>[$]<br>\begin{tabular}{ c | c }<br>Population Quantity & Sample Counterpart \\<br>\hline<br>probability distribution & histogram \\<br>(population) mean, \mu & (sample) mean, \bar{x} \\<br>(population) variance, \sigma^2 & (sample) variance, s^2 \\<br>(population) proportion, \pi & (sample) proportion, p<br>\end{tabular}<br>[/$]
Sampling distribution of [$]\bar{X}[/$] for normal distributions	When taking a random sample of size [$]n[/$] from a [$]N(\mu, \sigma^2)[/$] population, it can be shown that exactly:<br><br>[$]\bar{X} \sim N(\mu, \frac{\sigma^2}{n})[/$]<br><br>As per the central limit theorem, [$]\bar{X}[/$] will be approximately normal when sampling from a non-normal population, as [$]n \to \infty[/$]
The standard deviation of [$]\bar{X}[/$], ie. the standard error	[$]\sqrt{Var(\bar{X})} = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}[/$]
Definition of 'point estimate'	A point estimate is our best guess of an unknown population parameter based on sample data.  For example, the sample mean, [$]\bar{x}[/$], is a point estimator of the population mean, [$]\mu[/$].
Definition of 'standard errors'	Standard errors act as measures of estimation (im)precisions and these are used in the construction of confidence intervals.
Definition of 'confidence interval'	Formally, a x% confidence interval covers the unknown parameter, with x% probability over repeated samples.
Definition of 'coverage probability'	The coverage probability of a technique for calculating a confidence interval is the proportion of the time that the interval contains the true value of interest.
Student's t Distribution	Given a random sample of [$]n[/$] distributions, with mean [$]\bar{x}[/$] and standard deviation [$]s[/$], from a normally distributed population with mean [$]\mu[/$], the random variable t follows the student's t distribution with [$](n - 1)[/$] degrees of freedom, and is given by:<br><br>[$]t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}[/$]<br><br>Note that as degrees of freedom approaches infinity the distribution approaches the standard normal distribution.
Confidence interval endpoints from a single mean ([$]\sigma[/$] known)	In such instances, we use the standard normal distribution when constructing a confidence interval, with the endpoints:<br><br>[$]\bar{x} \pm z_\frac{\alpha}{2} \times \frac{\sigma}{\sqrt{n}}[/$]<br><br>Where [$]z_\frac{\alpha}{2}}[/$] is the z-value which cuts off [$]\frac{\alpha}{2}[/$] probability in the upper tail of the standard normal distribution, to ensure a [$]100(1 - \alpha)\%[/$] probability.
Confidence interval endpoints for a single mean ([$]\sigma[/$] unknown)	In such instances we use the t distribution when constructing a confidence interval with the endpoints:<br><br>[$]\bar{x} \pm t_\frac{\alpha}{2}} \times \frac{s}{\sqrt{n}}[/$]
Sampling distribution of the sample proportion estimator, [$]p[/$]	The sampling distribution of the sample proportion, [$]p[/$], which is the estimator of the population proportion [$]\pi[/$], is:<br><br>[$]p \sim N(\pi, \frac{\pi(1 - \pi)}{n})[/$]<br><br>[$]\pi[/$] is unknown and what we are trying to estimate, and therefore we do not know the true standard error. In this case the estimated standard error is:<br><br>[$]ESE(p) = \sqrt{\frac{p(1 - p)}{n}} = \sqrt{\frac{\frac{r}{n}(1 - \frac{r}{n})}{n}}[/$]
Confidence interval endpoints for a single proportion	[$]p \pm z_\frac{\alpha}{2} \times \sqrt{\frac{p(1 - p)}{n}}[/$]
Why do we not use student's t distribution for confidence interval endpoints for a single proportion?	We do not use student's t distribution for two reasons:<br><br>1. The standard error has not been estimated by a corrected sum of squares calculation, unlike [$]s^2 = \frac{S_{xx}}{(n - 1)}[/$]<br><br>2. The sample size, [$]n[/$], has to be large for the normal approximation to hold, and so the normal approximation is appropriate in this case
Sample size determination for a single mean	To estimate [$]\mu[/$] to within [$]e[/$] units, with a [$]100(1 - \alpha)\%[/$] confidence, we require a sample of:<br><br>[$]n \geq \frac{(z_\frac{\alpha}{2})^2 \times \sigma^2}{e^2}[/$]
Sample size determination for a single proportion	To estimate [$]\pi[/$] to within [$]e[/$] units, with a [$]100(1 - \alpha)\%[/$] confidence, we require a sample of:<br><br>[$]n \geq \frac{(z_\frac{\alpha}{2})^2 \times p(1 - p)}{e^2}[/$]<br><br><br>and remember that [$]p = \frac{r}{n}[/$]
Confidence interval endpoints for the difference between two proportions	With point estimates for [$]\pi_1[/$] and [$]\pi_2[/$] in the form of [$]p = \frac{r}{n}[/$]<br><br>[$](p_1 - p_2) \pm z_\frac{\alpha}{2} \times \sqrt{\frac{p_1(1 - p_1)}{n_1} + \frac{p_2(1 - p_2)}{n_2}}[/$]
What are the four cases to be considered when computing the difference between two population means?	1. Unpaired samples - variances known<br>2. Unpaired samples - variances unknown and unequal<br>3. Unpaired samples - variances unknown and equal<br>4. Paired (dependent) samples
Confidence interval endpoints for the difference between two means with: unpaired samples - variences known (ie. [$]\sigma_1^2[/$] and [$]\sigma_2^2[/$])	[$](\bar{x_1} - \bar{x_2}) \pm z_\frac{\alpha}{2} \times \sqrt{ \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}[/$]
Confidence interval endpoints for the difference between two means with: variances unknown and unequal	If [$]\sigma_1^2[/$] and [$]\sigma_2^2[/$] are unknown, provided sample sizes [$]n_1[/$] and [$]n_2[/$] are large:<br><br>[$](\bar{x_1} - \bar{x_2}) \pm z_\frac{\alpha}{2} \times \sqrt{ \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}[/$]
Pooled varience estimator for when: samples are unpaired and variences are unknown and equal	When the common varience [$]\sigma^2[/$] is unknown, but we can assume that the two populations being sampled are of equal variability, we can use both [$]s_1^2[/$] and [$]s_2^2[/$] to estimate the value of [$]\sigma^2[/$] where:<br><br>[$]s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}[/$]
Confidence interval endpoints for the differnce between two means with: samples unpaired and variences unknown and equal	We use [$]s_p^2[/$], the pooled varience estimator, to compute:<br><br>[$](\bar{x_1} - \bar{x_2}) \pm t_{\frac{\alpha}{2} , n_1 + n_2 - 2} \times \sqrt{s_p^2 \times (\frac{1}{n_1} + \frac{1}{n_2})}[/$]
If it is not explicitly stated that the two variences are equal in the problem, we can check by considering the following two axioms:	1. If [$]\sigma_1^2 = \sigma_2^2[/$], we would expect approximately equal sample variances [$]s_1^2 \approx s_2^2[/$] and if they are very different than [$]\sigma_1^2 \neq \sigma_2^2[/$]<br><br>2. If we are sampling two similar populations (eg. companies in the same industry) then it is reasonable to assume equal variability.
Confidence interval endpoints for the difference between two means when: pooled (dependent) samples	This is for observations of the same individuals in two different states (ie. before and after), where [$]n_1[/$] must be equal to [$]n_2[/$]<br><br>We can simply reduce this to differenced data, ie. if a is before and b is after then:<br><br>[$]\sum_{i=1}^n d_i = a_i - b_i[/$]<br>and<br>[$]\bar{x_d} = \bar{a} - \bar{b}[/$]<br><br>And therefore, to compute the confidence interval of the mean:<br><br>[$]\bar{x_d} = t_{\frac{\alpha}{2} , n - 1} \times \frac{s}{\sqrt{n}}[/$]
Four considerations for selecting the student's t distribution, versus the standard normal distribution	1. We use the standard normal distribution when we know the varience or the standard deviation<br>2. If we need to estimate the varience or the standard deviation, we use the student's t distribution regardless of the size of the sample<br>3. When the sample size is large (>30), then the standard normal distribution approximates the student's t distribution<br>4. We use the standard normal distribution whenever we are dealing with (unpaired) proportions
Definition of the 'null hypothesis'	The null hypothesis is our 'working hypothesis', which we will assume to be true until we  obtain significant evidence against it.  In ST104A, the hypothesis will only contain parameters and the null hypothesis will always denote the parameter value with equality.  (This is called the simple null hypothesis).
Definition of the 'test statistic'	The formal mechanism used to evaluate the support given to the null hypothesis by sample data. Using the given sample data, we evaluate the test statistic to obtain a test statistic value.<br><br>A common test statistic is in the form:<br>(Point Estimator - Hypothesized Value) / (Estimated) Standard Error<br><br>aka<br><br>[$]\frac{\hat{\theta} - \theta_0}{S.E.(\hat{\theta})} \sim N(0,1)[/$]
Definition of the 'critical region'	The area on the distribution, defined by one or two critical values, where we reject the null hypothesis if the test statistic value falls within it. This is called the decision rule.
Two types of errors that can be commited in any hypothesis test	1. A type I error: rejecting the null hypothesis when it is true, (aka a false positive). Denote the probability of this type of error by [$]\alpha[/$]<br><br>2. A type II error: Failing to reject the null hypothesis when it is false, (aka a false negative). Denote the probability of this type of error by [$]\beta[/$]
Steps for performing a hypothesis test for normal populations	1. Define the hypothesis<br>2. State the test statistic and its distribution, then compute its value<br>3. Define the critical region for a given significance level<br>4. Decide whether or not to reject the null hypothesis<br>5. Draw conclusions
Definiton of the 'significant level'	The significance level, or size of a test, [$]\alpha[/$], is the probability of commiting a type I error.
Definition of the 'p-value'	The p-value is the probability of obtaining the observed test statistic value, or a more extreme value, conditional on the null hypothesis being true.<br><br>Calculating the p-value depends on the form of [$]H_a[/$] and the statistic distribution:<br><br>[$]<br>\begin{tabular}{ c|c }<br>form of H_1 & p-value calculation \\<br>\hline<br>H_a: \theta \neq \theta_0 & 2 \times P (X \geq | x |) \\<br>H_a: \theta < \theta_0 & P(X \leq x) \\<br>H_a: \theta > \theta_0 & P(X \geq x) \\<br>\end{tabular}<br>[/$]
z-test of a hypothesis for a single mean ([$]\sigma[/$] known)	[$]Z = \frac{\bar{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)[/$]
z-test of a hypthesis for a single mean ([$]\sigma<\$> unknown)	[$]Z \cong \frac{\bar{X} - \mu_0}{\frac{S}{\sqrt{n}}} \sim t_n - 1[/$]
z-test of a hypothesis for a single proportion	[$]Z \cong \frac{p - \pi_0}{\sqrt{\frac{\pi_0(1 - \pi_0)}{n}}} \sim N(0,1)[/$]
Pooled proportion estimator	If [$]R_1[/$] and [$]R_2[/$] represent the number of 'favourable' responses from two independent samples, with sample sizes [$]n_1[/$] and [$]n_2[/$], then the pooled proportion estimator is:<br><br>[$]P = \frac{R_1 + R_2}{n_1 + n_2}[/$]
z-test for the difference between two proportions	[$]Z \cong \frac{(P_1 - P_2) - (\pi_1 - \pi_2)}{\sqrt{P(1 - P) \times (\frac{1}{n_1} + \frac{1}{n_2})}} \sim N(0,1)[/$]<br><br>OR<br><br>[$]Z \cong \frac{(P_2('after') - P_1('before')}{\sqrt{P(1 - P) \times (\frac{1}{n_1} + \frac{1}{n_2})}} \sim N(0,1)[/$]
z-test for the difference between two means (variances known)	[$]Z = \frac{\bar{X_1} - \bar{X_2} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim N(0,1)[/$]
z-test for the difference between two means (variences unknown)	[$]Z \cong \frac{\bar{X_1} - \bar{X_2} - (\mu_1 - \mu_2)}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}} \sim N(0,1)[/$]
t-test for the difference between two means (variences unknown)	[$]T = \frac{\bar{X_1} - \bar{X_2} - (\mu_1 - \mu_2)}{\sqrt{S_p^2(\frac{1}{n_1} + \frac{1}{n_2})}} \sim t_{n_1 + n_2 - 2}[/$]<br><br>Where [$]S_p^2[/$] is the pooled varience estimator
t-test for the difference in means for paired (dependent) samples	eg. before and after statistics for the same sample<br><br>[$]T = \frac{\bar{X_d} - \mu_d}{\frac{S_d}{\sqrt{n}}} \sim t_{n - 1}[/$]
Definition of 'non-parametric statistics'	Non-parametric statistics refers to a statistical method in which the data is not required to fit a normal distribution. It often uses data that is ordinal or categorical.
Definition of 'correlation'	Correlation, for ST104A, is defined as 'a measure of strength of the linear relationship between two random variables'
Expected frequencies in contingency tables	[$]E_{ij} = \frac{(row\ i\ total)(row\ j\ total)}{(total\ number\ of\ observations)}[/$]<br><br>From this we can produce an expected frequency table, which is a step toward performing a [$]\chi^2[/$] test of association.
Pearson's [$]\chi^2[/$] test of association	[$]\sum_{i=1}^r\sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}} \sim \chi^2_{(r - 1)(c - 1)}[/$]<br><br>Where r and c denote the number of rows and columns.<br><br>Note: [$]\chi^2[/$] tests of association are always upper-tailed tests.
Expected frequencies in goodness-of-fit tests	[$]E_i = n \times \frac{1}{k}\ , i = 1, 2, ..., k[/$]<br><br>Where [$]n[/$] denotes the sample size and [$]\frac{1}{k}[/$] is the uniform (equal, same) probability for each characteristic.<br><br>In all goodness-of-fit tests, the data must be expressed in the form of observed frequencies.
Goodness-of-fit test statistic	For a discrete, uniform distribution, with [$]k[/$] categories, observed frequencies [$]O_i[/$] and expected frequencies [$]E_i[/$], the test statistic is:<br><br>[$]\sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i} \sim \chi^2_{k-1}[/$]<br><br>Resulting in an approximate distribution under [$]H_0[/$] only if all of the expected are at least five. (Merge the categories if [$]E_i[/$] is less than five).
Why is obtaining a total enumeration of the population so difficult?	A census (ie. the total enumeration of the population) is difficult to administer, time-consuming, expensive, and does not guarantee completely accurate results, due to non-sampling errors.
'Primary data' vs. 'non-primary data'	Primary data are new data collected by researchers for a particular purpose.<br><br>Non-primary data refers to existing data which has already been collected by others for another purpose.
Two top-level types of sample	1.) Non-probability (non-random) sample, which can be further broken into three types:<br>a. convenience sampling<br>b. judgement sampling<br>c. quota sampling<br>2.) probability (random) sample, which can further be broken into five types:<br>a. simple random sampling<br>b. systematic random sampling<br>c. stratified random sampling<br>d. cluster sampling<br>e. multi-stage sampling
Four properties that characterise non-probability samples	1. Some population units have no chance (ie. zero probability) of being selected<br>2. Units which can be selected, have an unknown (non-zero) probability of selection<br>3. Sampling errors cannot be quantified<br>4. Non-probability samples are used in the absence of a sampling frame
How is convenience sampling performed?	Individuals are selected from the members of the population that happen to be in the vicinity
How is judgement sampling performed?	Judgement sampling uses expert opinion to judge which individuals to choose (eg. querying if participants fit a target profile to join a focus group)
How is quota sampling performed?	Quota sampling attempts to obtain a representative sample by specifying quota controls on certain characteristics, such as age, gender, etc...<br><br>The approximate distribution of these characteristics in the population should be known in advance, in order to replicate it in the sample.
Why can we not use confidence intervals or hypothesis tests when performing non-probability sampling?	We cannot use confidence intervals or hypothesis tests becayse we do not know the probability that an individual will be selected. Therefore, standard errors are not measurable.
Definition of 'simple random sampling'	A sampling method where each unit has a known, equal, and non-zero probability of selection.
Definition of 'systematic random sampling'	A [$]1-in-x[/$] systematic random sample is obtained by randomly selecting one of the first [$]x[/$] units in the sampling frame, and then selecting every subsequent [$]x^{th}[/$] unit.<br><br>Note that it is important to consider if there is an inherent periodicity in the data that would bias the sample.
How do we collect a  'stratified random sample'?	A simple random sample is taken from pre-identified stratas - groupings of individuals characterized as having population units which are similar within strata, but different between strata.
Definition of 'multi-stage sampling'	Multi-stage sampling refers to a cluster sample when sample selection occurs at two or more successive stages.<br><br>(This is often used in large surveys).
How do we collect a 'cluster sample'?	The population is divided into clusters, so that each cluster is as variable as the overall population (ie. heterogeneity within clusters and homogeneity between clusters).  Then, some of the clusters are selected by simple random surveying.<br><br>A one-stage cluster sample is when every unit in the chosen clusters are surveyed.<br><br>A two-stage cluster sample is when selecting a simple random sample from the clusters that were already selected by a simple random sample (aka a multi-stage design).
What are the similarities and differences between 'stratified' and 'cluster' sampling?	- Stratified sampling: all strata are chosen, some units selected in each stratum<br>- One-stage cluster sampling: Some clusters are chosen, all units selected in each sampled cluster.<br>- Two-stage cluster sampling: Some clusters are chosen, some units are selected in each sampled cluster.
What are the two types of errors in sampling design?	1.) Sampling error: results from random variation due to the sampling scheme. For probability sampling we can estimate the statistical properties of the sampling error (ie. we can compute the standard errors).<br>2.) Non-sampling error: occurs as a result of the failures of the sampling scheme. Sampling errors cannot be quantified for non-probability sampling.
What are the two sub-types of non-sampling error?	1.) Selection bias, which may be due to: 1) the sampling frame not being equal to the target population; 2) the sampling frame non being strictly adhered to; or, 3) non-response bias.<br>2.) Response bias, where the actual measurements might be incorrect because of interviewer bias, ambiguous questions, misunderstandings, sensitive information, ...
What are two uses of a pilot survey?	1.) To find the standard error that can be attached to different kinds of questions.<br>2.) To sort out non-sampling questions, with regards to misunderstandings, interviewer bias, ...
What are the two classifications of non-response?	1.) Item non-response: when a sampled member fails to respond to a question in the questionnaire.<br>2.) Unit non-response: when no information is collected from a sample member.
What are two sources of response error?	1.) The role of the interviewer: askng leading questions and the incorrect recording of responses.<br>2.) Role of the respondent: may lack knowledge, forget information or be reluctant to give the right answer, due to sensitivity.
Three methods of contact	1.) Face-to-face interviews<br>2.) Telephone interviews<br>3.) Self-completion interviews<br><br>With increasing levels of non-response bias and decreasing levels of expense.
Two methods statisticians use to try to measure causal relationships in the social sciences	1. The control group<br>2. Time order
Definition of the 'control group'	The control group is a sample set that is observed without any intervention. In an experiment, the allocation of a treatment/intervention is determined using a form of randomization and the control group does not recieve the treatment. In a blind experiment, the experimental units are unaware of whether they have recieved the treatment (or a placebo). In a double-blind experiment, the administrators are unaware of who has recieved the treatment.
Two types of 'time order' surveys.	1. Longitudinal surveys: study sample populations over a very long period of time. These studies can actually measure individual changes, but subjects may drop out over time or be conditioned by the researcher. Often used by policy makers.<br><br>2. Panel surveys: also involve contacting the same individuals over a time peroid, but differ from longitudinal surveys in three ways:<br>a. participants are more likely to be chosen by quota, rather than random methods<br>b. individuals are interviewed every 2 to 4 weeks (rather than years)<br>c. individuals are unlikely to be panel members for > 2 years at a time
The difference between correlation and regression	Correlation measures the strength of a linear (for ST104A) relatonship<br><br>Regression is a way of representing that linear relationship
Sample correlation coefficient	[$]r = \frac{S_{xy}}{\sqrt{S_{xx} \times S_{yy}}} = \frac{\sum_{i=1}^n x_iy_i - n\bar{x}\bar{y}}{\sqrt{(\sum_{i=1}^nx_i^2 - n\bar{x}^2)(\sum_{i=1}{n}y_i^2 - n\bar{y}^2)}}[/$]<br><br>You will likely be given to following summary statistics on an exam:<br>[$]S_{xx}[/$] = the corrected sum of squares of x-data<br>[$]S_{yy}[/$] = the corrected sum of squares of y-data<br>[$]S_{xy}[/$] = the corrected sum of cross-products
Four properties of the correlation coefficient	1. It is independent of the scale of measurement<br>2. It is independent of the origin of measurement<br>3. It is symmetric; that is, the correlation of X and Y is the same as the correlation of Y with respect to X<br>4. It can only take values between [$]\pm1[/$], ie. [$]|p| or |r| \leq 1[/$]
The meanings of high, low and zero correlation coefficients	A high (ie. near +1) coefficient indicates a strong, positively correlated relationship.<br>A low, negative (ie. near -1) coefficient indicates a strong, negatively correlated relationship.<br>A coefficient of zero indicates that X and Y are not linearly related.
Spearman rank correlation	[$]r_s = 1 - \frac{6\sum_{i=1}^nd_i^2}{n(n^2 - 1)}[/$]<br><br>Where the [$]d_i[/$] terms are the difference in the ranks between each [$]x_i[/$] and [$]y_i[/$]
Four assumptions of the simple linear regression model	1. A linear relationship between the variables of the form [$]y = \alpha + \beta x + \epsilon[/$]<br>2. The existence of 3 model parameters: the linear equation parameters [$]\alpha[/$] and [$]\beta[/$] and the variance [$]\sigma^2[/$]<br>3. [$]Var(\epsilon_i) = \sigma^2[/$] for all [$]i = 1, ..., n[/$]<br>ie. it does not depend on the explanatory variable, (aka the independent variable), x<br><br>4. The [$]E_i[/$] terms are independent and [$]N(0, \sigma^2)[/$] distributed for all [$]i = 1, ..., n[/$]
Simple linear regression line estimates	We estimate [$]\alpha[/$] and [$]\beta[/$] with [$]a[/$] and [$]b[/$], where:<br><br>[$]b = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}[/$]<br>[$] = \frac{\sum_{i=1}^nx_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2 - n\bar{x}^2} = \frac{S_{xy}}{S_{xx}}[/$]<br><br>and<br><br>[$]a = \bar{y} - b \bar{x}[/$]
Simple linear regression: predicting [$]y[/$] for a given value of [$]x[/$]	For a given value of the exploratory variable, [$]x_0[/$], the predicted value of the dependent variable [$]\hat{y} = a + bx_0[/$]<br><br>(Remember to include the appropriate units!)
Definition of 'extrapolation	Extrapolation is when you predict [$]x_0[/$] values that lie outside of the range of available [$]x[/$] data. These predictions should be viewed with caution.
